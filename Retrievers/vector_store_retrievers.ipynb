{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector Store Retriever "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain_core.documents import Document\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Your Source Documents\n",
    "\n",
    "documents = [\n",
    "    Document(page_content = \"Langchain helps developers build LLM applications easily\"),\n",
    "    Document(page_content = \"Chroma is a vector database optimized for LLM-based search\"),\n",
    "    Document(page_content = 'Embeddings convert text into high-dimensional vectors'),\n",
    "    Document(page_content = 'GenAI provides powerful embedding models.'),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2 : Intialize embedding model\n",
    "\n",
    "embedding_model = GoogleGenerativeAIEmbeddings(model = 'models/gemini-embedding-001')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3 : Create Chroma vector store in memory\n",
    "\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents = documents,\n",
    "    embedding = embedding_model,\n",
    "    collection_name = 'my_collection'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Convert vectorstore into a retriever\n",
    "\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\":2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "query  =  \"What is chroma used for\"\n",
    "results = retriever.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Result 1---\n",
      "Chroma is a vector database optimized for LLM-based search\n",
      "\n",
      "--- Result 2---\n",
      "GenAI provides powerful embedding models.\n"
     ]
    }
   ],
   "source": [
    "for i, doc in enumerate(results):\n",
    "    print(f\"\\n--- Result {i+1}---\")\n",
    "    print(doc.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### we get different different search strategy implimentation too here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximum Marginal Relevance (MMR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. \"**How can we pick results that are not only relevant to the query but also different from each other**\"\n",
    "2. MMR is an information retieval algorithm designed to reduce redundancy in the retrieved results while maintaining high relevance to the query.\n",
    "\n",
    "### Why MMR Retriever\n",
    "\n",
    "In regular similarity search you may get documents that are \n",
    "1. All very similar to each other.\n",
    "2. Repeating the same info\n",
    "3. Lacking deliver perspectives\n",
    "\n",
    "MMR Review avoids that by\n",
    "1.  Picking the most relevant document first\n",
    "2. Then picking the **most relevant and least similar** to already docs\n",
    "3. and so on\n",
    "\n",
    "This helps especially in RAG pipelines where\n",
    "1. You want your context window to contain diverse but still relevant information\n",
    "2. Especially useful when documents are **semantically overlapping**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical in Python (MMR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample documents\n",
    "\n",
    "docs = [\n",
    "    Document(page_content = \"Langchain makes it easy to work with LLMs.\"),\n",
    "    Document(page_content = \"Langchain is used to buld LLM based applications\"),\n",
    "    Document(page_content = \"Chroma is used to store and search document embeddings\"),\n",
    "    Document(page_content = 'Embeddings are vector represetation of text.'),\n",
    "    Document(page_content = 'Embeddings are vector representations of text.'),\n",
    "    Document(page_content = \"MMR helps you get diverse results when doing similarity search.\"),\n",
    "    Document(page_content = \"Langchain supports Chroma, FAISS, Pinecone, and more.\")\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize GoogleGenAI embeddings\n",
    "\n",
    "embedding_model = GoogleGenerativeAIEmbeddings(\n",
    "    model = 'models/gemini-embedding-001'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2 -> Create the FAISS store from documents\n",
    "\n",
    "vectorstore = FAISS.from_documents(\n",
    "    documents = docs,\n",
    "    embedding= embedding_model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3 -> Enable MMR in the retriever\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type = 'mmr',\n",
    "    search_kwargs = {'k' : 3, 'lambda_mult': 1} # k = top results, lambda_mult = relevance-diversity balance\n",
    ") # less the lamba_mult, more the diversity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is langchain ?\"\n",
    "results = retriever.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " --- Result1 ---\n",
      "Langchain is used to buld LLM based applications\n",
      "\n",
      " --- Result2 ---\n",
      "Langchain makes it easy to work with LLMs.\n",
      "\n",
      " --- Result3 ---\n",
      "Langchain supports Chroma, FAISS, Pinecone, and more.\n"
     ]
    }
   ],
   "source": [
    "for i, doc in enumerate(results):\n",
    "    print(f\"\\n --- Result{i+1} ---\")\n",
    "    print(doc.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Multi-Query Retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sometimes a single query might not capture all the ways information in phrased in your documents\n",
    "\n",
    "For Example:\n",
    "\n",
    "Query :\n",
    "\"How can i stay health\"\n",
    "\n",
    "Could mean \n",
    "\n",
    ". What should i eat "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
